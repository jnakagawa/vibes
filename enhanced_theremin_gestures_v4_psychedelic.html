<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8"/>
<meta name="viewport" content="width=device-width, initial-scale=1.0"/>
<title>Simplified Gesture Theremin v4 - Lightweight Effects</title>
<style>
  html, body { margin:0; padding:0; overflow:hidden; background:black; color:white; }
  #controls {
    position:absolute; top:10px; left:10px;
    background:rgba(0,0,0,0.8); padding:15px; border-radius:10px;
    font-family:sans-serif; z-index:5; max-width:450px;
    border: 2px solid #333;
    backdrop-filter: blur(10px);
  }
  #controls label { display:block; margin-top:8px; font-size:12px; }
  #controls input[type="range"] { width:100%; }
  #controls select { width:100%; margin-top:4px; }
  #start-btn { margin-top:8px; padding:8px 12px; background:#4CAF50; color:white; border:none; border-radius:5px; cursor:pointer; }
  #start-btn:hover { opacity:0.8; }
  canvas { position:absolute; top:0; left:0; width:100%; height:100%; }
  .status-indicator {
    display: inline-block; width: 12px; height: 12px; border-radius: 50%;
    margin-right: 8px; background: #ff4444;
  }
  .status-indicator.active { background: #44ff44; }
  .hand-info {
    background: rgba(255,255,255,0.1); padding: 8px; border-radius: 5px;
    margin-top: 10px; font-size: 11px;
  }
  .progress { background: rgba(255,255,255,0.2); padding: 5px; border-radius: 3px; margin-top: 5px; }
  .gesture-status {
    background: rgba(100,255,100,0.2); padding: 8px; border-radius: 5px;
    margin-top: 10px; font-size: 12px; border: 1px solid #4f4;
  }
  .active-sounds {
    background: rgba(255,200,0,0.2); padding: 6px; border-radius: 5px;
    margin-top: 8px; font-size: 11px; border: 1px solid #fc0;
  }
  .visual-controls {
    background: rgba(255,0,255,0.2); padding: 8px; border-radius: 5px;
    margin-top: 8px; font-size: 11px; border: 1px solid #f0f;
  }
</style>
</head>
<body>
<div id="controls">
  <div style="display: flex; align-items: center; margin-bottom: 10px;">
    <h2 style="margin: 0; flex: 1;">üéµ Simplified Theremin v4</h2>
    <div class="status-indicator" id="model-status"></div>
  </div>
  
  <button id="start-btn">Start Audio & Simple Visuals</button>
  
  <div class="hand-info">
    <div>üé≠ <strong>Step 4:</strong> Lightweight Visual Effects</div>
    <div id="hand-detection-status">Loading ML5.js HandPose model...</div>
    <div class="progress">
      ‚úÖ Simple hand tracking<br>
      ‚úÖ Pitch-based colors<br>
      ‚úÖ Lightweight performance<br>
      ‚úÖ All gesture controls active
    </div>
  </div>
  
  <div class="gesture-status">
    <strong>ü§ö Gesture Controls:</strong><br>
    ‚Ä¢ Open Hand: Start new sound<br>
    ‚Ä¢ ‚úä Fist: Hold/sustain current sound<br>
    ‚Ä¢ ‚úåÔ∏è Peace: Stop all sounds<br>
    <div id="gesture-feedback">Show hands to see gestures...</div>
  </div>

  <div class="active-sounds">
    <strong>üéµ Active Sound Layers:</strong>
    <div id="sound-layers">None</div>
  </div>

  <div class="visual-controls">
    <strong>üåà Visual Effects:</strong><br>
    <label>Effect Intensity: <span id="effectIntensityVal">0.7</span>
      <input type="range" id="effectIntensity" min="0.1" max="1" step="0.1" value="0.7">
    </label>
    <label>Hand Glow Size: <span id="handGlowVal">30</span>
      <input type="range" id="handGlow" min="10" max="60" step="5" value="30">
    </label>
  </div>

  <div class="visual-controls" style="border-color: #0ff;">
    <strong>üìè Volume Calibration:</strong><br>
    <label>Close Hand Distance (100% vol): <span id="maxDistanceVal">120</span>
      <input type="range" id="maxDistance" min="100" max="800" step="10" value="120">
    </label>
    <label>Far Hand Distance (0% vol): <span id="minDistanceVal">40</span>
      <input type="range" id="minDistance" min="10" max="200" step="5" value="40">
    </label>
    <div id="distance-debug" style="font-size: 10px; color: #0ff; margin-top: 5px;">
      Current distances will appear here...
    </div>
  </div>
  
  <h3>Left Hand LFO Instrument</h3>
  <label>Waveform:
    <select id="leftWaveform">
      <option value="sine">Sine Wave</option>
      <option value="triangle">Triangle Wave</option>
      <option value="sawtooth">Sawtooth Wave</option>
    </select>
  </label>
  <label>Base Pitch: <span id="leftBasePitchVal">440</span> Hz
    <input type="range" id="leftBasePitch" min="100" max="1000" value="440">
  </label>
  <label>Y-Pitch Range: <span id="leftPitchRangeVal">200</span> Hz
    <input type="range" id="leftPitchRange" min="50" max="500" value="200">
  </label>
  <label>LFO Depth: <span id="leftLfoDepthVal">50</span> Hz
    <input type="range" id="leftLfoDepth" min="0" max="200" value="50">
  </label>
  <label>Max Volume: <span id="leftMaxGainVal">0.3</span>
    <input type="range" id="leftMaxGain" min="0" max="1" step="0.01" value="0.3">
  </label>

  <h3>Right Hand White Noise</h3>
  <label>Max Volume: <span id="rightMaxGainVal">0.25</span>
    <input type="range" id="rightMaxGain" min="0" max="1" step="0.01" value="0.25">
  </label>
  <label>Y-Pitch Range: <span id="rightPitchRangeVal">2000</span> Hz
    <input type="range" id="rightPitchRange" min="500" max="5000" value="2000">
  </label>
  <label>Min Pitch: <span id="rightMinPitchVal">200</span> Hz
    <input type="range" id="rightMinPitch" min="50" max="1000" value="200">
  </label>
  <label>Reverb Wet: <span id="rightReverbWetVal">0.5</span>
    <input type="range" id="rightReverbWet" min="0" max="1" step="0.01" value="0.5">
  </label>

  <h3>Global Settings</h3>
  <label>Master Volume: <span id="masterVolumeVal">0.7</span>
    <input type="range" id="masterVolume" min="0" max="1" step="0.01" value="0.7">
  </label>
  <label>Smoothing: <span id="smoothingVal">0.15</span>
    <input type="range" id="smoothing" min="0.01" max="0.5" step="0.01" value="0.15">
  </label>
</div>

<video id="video" autoplay playsinline style="display:none"></video>
<canvas id="video-canvas"></canvas>
<canvas id="overlay-canvas"></canvas>

<!-- ML5.js library -->
<script src="https://unpkg.com/ml5@latest/dist/ml5.min.js"></script>

<script>
(async () => {
  // Debug logging
  function debugLog(message, data = null) {
    console.log(`[SimplifiedTheremin-v4] ${message}`, data || '');
  }

  debugLog('Initializing Simplified Gesture Theremin v4 - Lightweight Effects');

  // DOM elements
  const video = document.getElementById('video');
  const vcanvas = document.getElementById('video-canvas'), vctx = vcanvas.getContext('2d');
  const ocanvas = document.getElementById('overlay-canvas'), octx = ocanvas.getContext('2d');
  const startBtn = document.getElementById('start-btn');
  const modelStatus = document.getElementById('model-status');
  const handDetectionStatus = document.getElementById('hand-detection-status');
  const gestureFeedback = document.getElementById('gesture-feedback');
  const soundLayers = document.getElementById('sound-layers');

  // Control elements
  const ctrl = {};
  ['leftWaveform', 'leftBasePitch', 'leftPitchRange', 'leftLfoDepth', 'leftMaxGain',
   'rightMaxGain', 'rightPitchRange', 'rightMinPitch', 'rightReverbWet',
   'masterVolume', 'smoothing', 'effectIntensity', 'handGlow', 'maxDistance', 'minDistance'].forEach(id => {
    ctrl[id] = document.getElementById(id);
  });

  // Additional DOM elements for calibration
  const distanceDebug = document.getElementById('distance-debug');

  // Bind control values to display spans
  function bind(id, spanId) {
    const inp = document.getElementById(id), span = document.getElementById(spanId);
    if (!inp || !span) return;
    span.innerText = inp.value;
    inp.oninput = () => span.innerText = inp.value;
  }

  // Bind all controls
  ['leftBasePitch', 'leftPitchRange', 'leftLfoDepth', 'leftMaxGain',
   'rightMaxGain', 'rightPitchRange', 'rightMinPitch', 'rightReverbWet',
   'masterVolume', 'smoothing', 'effectIntensity', 'handGlow', 'maxDistance', 'minDistance'].forEach(id => {
    bind(id, id + 'Val');
  });

  // Canvas sizing
  let width = window.innerWidth, height = window.innerHeight;
  [vcanvas, ocanvas].forEach(c => { c.width = width; c.height = height; });
  
  // EDGE DETECTION MOTION TRACKING SYSTEM - Initialize early
  let frameCount = 0;
  
  // Create offscreen canvas for edge detection
  const offCanvas = document.createElement('canvas');
  const offCtx = offCanvas.getContext('2d');
  let previousFrameData = null;

  // Initialize offscreen canvas
  function initializeEdgeDetection() {
    offCanvas.width = width;
    offCanvas.height = height;
    debugLog('‚úÖ Edge detection system initialized');
  }

  window.addEventListener('resize', () => {
    width = window.innerWidth; 
    height = window.innerHeight;
    [vcanvas, ocanvas].forEach(c => { c.width = width; c.height = height; });
    // Also resize offscreen canvas for edge detection
    offCanvas.width = width;
    offCanvas.height = height;
    debugLog('Canvas resized', { width, height });
  });

  // Initialize edge detection system
  initializeEdgeDetection();

  // SIMPLIFIED VISUAL EFFECTS SYSTEM
  
  // Simple pitch to color mapping
  function pitchToColor(frequency, alpha = 1) {
    // Map frequency to hue (0-360 degrees)
    const minFreq = 80;
    const maxFreq = 2000;
    const normalizedFreq = Math.max(0, Math.min(1, (frequency - minFreq) / (maxFreq - minFreq)));
    
    // Create a rainbow mapping: low freq = red (0¬∞), high freq = violet (280¬∞)
    const hue = normalizedFreq * 280;
    
    return `hsla(${hue}, 80%, 60%, ${alpha})`;
  }

  // Simple background pulse effect based on audio activity
  function drawBackgroundPulse(activeSounds) {
    if (activeSounds.size === 0) return;
    
    const intensity = parseFloat(ctrl.effectIntensity.value);
    let totalVolume = 0;
    let avgPitch = 440;
    
    activeSounds.forEach(sound => {
      totalVolume += sound.params.volume.value;
      avgPitch = sound.params.pitch.value; // Use last pitch for simplicity
    });
    
    if (totalVolume > 0.05) {
      // Simple pulsing background color
      const pulseAlpha = Math.sin(frameCount * 0.1) * intensity * totalVolume * 0.3;
      const color = pitchToColor(avgPitch, Math.abs(pulseAlpha));
      
      octx.fillStyle = color;
      octx.fillRect(0, 0, width, height);
    }
  }

  // EDGE DETECTION MOTION TRACKING SYSTEM
  
  // Edge detection visual effect
  function drawEdgeDetectionEffect() {
    if (!webcamRunning) return;
    
    // Draw current frame to offscreen canvas
    offCtx.drawImage(video, 0, 0, width, height);
    const currentFrameData = offCtx.getImageData(0, 0, width, height).data;
    
    if (previousFrameData) {
      const step = 30; // Sample every 30th pixel for performance
      const threshold = parseFloat(ctrl.effectIntensity.value) * 100; // Dynamic threshold
      
      for (let y = 0; y < height; y += step) {
        for (let x = 0; x < width; x += step) {
          const i = (y * width + x) * 4;
          
          // Calculate color difference between frames
          const rDiff = Math.abs(currentFrameData[i] - previousFrameData[i]);
          const gDiff = Math.abs(currentFrameData[i + 1] - previousFrameData[i + 1]);
          const bDiff = Math.abs(currentFrameData[i + 2] - previousFrameData[i + 2]);
          const totalDiff = rDiff + gDiff + bDiff;
          
          if (totalDiff > threshold) {
            // Create colorful motion circles
            const hue = (totalDiff / 765) * 360; // Map difference to hue
            const size = parseInt(ctrl.handGlow.value) * 0.5; // Use hand glow size control
            
            octx.beginPath();
            octx.arc(x, y, size, 0, 2 * Math.PI);
            octx.fillStyle = `hsla(${hue}, 100%, 50%, 0.7)`;
            octx.fill();
          }
        }
      }
    }
    
    // Store current frame for next comparison
    previousFrameData = new Uint8ClampedArray(currentFrameData);
  }

  // Draw all frozen sound layer circles
  function drawFrozenLayers() {
    activeSounds.forEach(sound => {
      if (sound.sustained && sound.frozenPosition) {
        const size = parseInt(ctrl.handGlow.value);
        
        // Draw frozen circle with distinctive styling
        octx.strokeStyle = sound.frozenColor;
        octx.lineWidth = 4;
        octx.setLineDash([5, 5]); // Dashed outline to show it's frozen
        octx.beginPath();
        octx.arc(sound.frozenPosition.x, sound.frozenPosition.y, size, 0, 2 * Math.PI);
        octx.stroke();
        octx.setLineDash([]); // Reset dash pattern
        
        // Inner fill with frozen styling
        octx.fillStyle = sound.frozenColor.replace('0.8)', '0.3)'); // More transparent
        octx.beginPath();
        octx.arc(sound.frozenPosition.x, sound.frozenPosition.y, size * 0.7, 0, 2 * Math.PI);
        octx.fill();
        
        // Freeze indicator
        octx.fillStyle = sound.frozenColor;
        octx.font = 'bold 16px Arial';
        octx.fillText('üßä', sound.frozenPosition.x - 8, sound.frozenPosition.y + 6);
        
        // Layer ID and frequency
        octx.font = 'bold 10px Arial';
        octx.fillStyle = sound.frozenColor;
        octx.fillText(`#${sound.id}`, sound.frozenPosition.x - 8, sound.frozenPosition.y - 25);
        octx.fillText(`${Math.round(sound.frozenFrequency)}Hz`, sound.frozenPosition.x - 15, sound.frozenPosition.y - 12);
      }
    });
  }

  // FIRST: Request camera access
  debugLog('Requesting camera access...');
  let webcamRunning = false;
  
  try {
    const stream = await navigator.mediaDevices.getUserMedia({ 
      video: { 
        width: 1280, 
        height: 720,
        facingMode: "user"
      } 
    });
    video.srcObject = stream; 
    await video.play();
    debugLog('‚úÖ Camera access granted and video stream started');
    webcamRunning = true;
    
  } catch (error) {
    debugLog('‚ùå Error accessing camera:', error);
    alert('Camera access failed. Please refresh and allow camera permissions.');
    handDetectionStatus.textContent = 'Camera access required';
    return;
  }

  // SECOND: Load ML5.js HandPose model
  debugLog('Loading ML5.js HandPose model...');
  handDetectionStatus.textContent = 'Loading ML5.js HandPose model...';
  
  let handpose = null;
  let predictions = [];

  function modelReady() {
    debugLog('‚úÖ ML5.js HandPose model loaded successfully');
    modelStatus.classList.add('active');
    handDetectionStatus.textContent = 'Simplified mode ready!';
    detectHands();
  }

  async function detectHands() {
    if (handpose && webcamRunning) {
      try {
        predictions = await handpose.detect(video);
      } catch (error) {
        debugLog('Error during detection:', error);
        predictions = [];
      }
    }
    setTimeout(detectHands, 100);
  }

  try {
    handpose = ml5.handPose({
      flipHorizontal: true,
      maxNumHands: 2,
      runtime: 'mediapipe',
      modelType: 'full'
    }, modelReady);
  } catch (error) {
    debugLog('‚ùå Error loading ML5.js HandPose model:', error);
    handDetectionStatus.textContent = 'Hand detection unavailable';
  }

  // GESTURE RECOGNITION FUNCTIONS (unchanged)
  function calculateHandDepth(keypoints) {
    if (!keypoints || keypoints.length < 21) return 0.5;
    
    const wrist = keypoints[0];
    const middleTip = keypoints[12];
    
    if (!wrist || !middleTip) return 0.5;
    
    const distance = Math.sqrt(
      Math.pow(middleTip.x - wrist.x, 2) + 
      Math.pow(middleTip.y - wrist.y, 2)
    );
    
    // Debug logging for calibration
    if (frameCount % 30 === 0) { // Log every 30 frames to avoid spam
      debugLog(`Hand distance: ${distance.toFixed(1)} pixels`);
    }
    
    // Improved calibration: close hands (large distance) = high volume
    // Far hands (small distance) = low volume
    const minDistance = 40;  // Very far hand (small appearance)
    const maxDistance = 120; // Very close hand (large appearance)
    
    // Map distance to 0-1 volume (larger distance = closer to camera = higher volume)
    const normalizedVolume = Math.max(0, Math.min(1, (distance - minDistance) / (maxDistance - minDistance)));
    
    return normalizedVolume;
  }

  function calculateHandDepthWithDistance(keypoints) {
    if (!keypoints || keypoints.length < 21) return { depth: 0.5, distance: 0 };
    
    const wrist = keypoints[0];
    const middleTip = keypoints[12];
    
    if (!wrist || !middleTip) return { depth: 0.5, distance: 0 };
    
    const distance = Math.sqrt(
      Math.pow(middleTip.x - wrist.x, 2) + 
      Math.pow(middleTip.y - wrist.y, 2)
    );
    
    // Use dynamic calibration controls
    const minDistance = parseFloat(ctrl.minDistance.value);  // Far hand (0% volume)
    const maxDistance = parseFloat(ctrl.maxDistance.value);  // Close hand (100% volume)
    
    // Map distance to 0-1 volume (larger distance = closer to camera = higher volume)
    const normalizedVolume = Math.max(0, Math.min(1, (distance - minDistance) / (maxDistance - minDistance)));
    
    return { depth: normalizedVolume, distance: distance };
  }

  function recognizeGesture(keypoints) {
    if (!keypoints || keypoints.length < 21) return 'unknown';
    
    const index_tip = keypoints[8];
    const middle_tip = keypoints[12];
    const ring_tip = keypoints[16];
    const pinky_tip = keypoints[20];
    
    const index_mcp = keypoints[5];
    const middle_mcp = keypoints[9];
    const ring_mcp = keypoints[13];
    const pinky_mcp = keypoints[17];
    
    const indexUp = (index_mcp.y - index_tip.y) > 30;
    const middleUp = (middle_mcp.y - middle_tip.y) > 30;
    const ringUp = (ring_mcp.y - ring_tip.y) > 20;
    const pinkyUp = (pinky_mcp.y - pinky_tip.y) > 20;
    
    const extendedFingers = [indexUp, middleUp, ringUp, pinkyUp].filter(Boolean).length;
    
    if (indexUp && middleUp && !ringUp && !pinkyUp) return 'peace';
    if (extendedFingers <= 1) return 'fist';
    if (extendedFingers >= 3) return 'open';
    return 'partial';
  }

  // Smooth value transitions (unchanged)
  class SmoothedValue {
    constructor(initialValue = 0, smoothing = 0.15) {
      this.value = initialValue;
      this.target = initialValue;
      this.smoothing = smoothing;
    }
    
    update(newTarget, customSmoothing = null) {
      this.target = newTarget;
      const alpha = customSmoothing !== null ? customSmoothing : this.smoothing;
      this.value = alpha * this.target + (1 - alpha) * this.value;
      return this.value;
    }
  }

  // LAYERED SOUND SYSTEM (simplified - no visual effects in sound layers)
  let activeSounds = new Map();
  let soundIdCounter = 0;
  let masterGainNode;

  class SoundLayer {
    constructor(id, type, hand) {
      this.id = id;
      this.type = type;
      this.hand = hand;
      this.active = false;
      this.sustained = false;
      
      // Visual properties for frozen representation
      this.frozenPosition = null;
      this.frozenColor = null;
      this.frozenFrequency = null;
      
      this.params = {
        pitch: new SmoothedValue(type === 'left' ? 440 : 400, 0.15),
        lfoRate: new SmoothedValue(0, 0.15),
        volume: new SmoothedValue(0, 0.15),
        reverb: new SmoothedValue(0, 0.15)
      };
      
      this.createAudioNodes();
    }
    
    createAudioNodes() {
      if (this.type === 'left') {
        this.osc = audioCtx.createOscillator();
        this.osc.type = ctrl.leftWaveform.value;
        this.gain = audioCtx.createGain();
        this.gain.gain.value = 0;
        
        this.lfo = audioCtx.createOscillator();
        this.lfo.type = 'sine';
        this.lfoGain = audioCtx.createGain();
        this.lfoDepthGain = audioCtx.createGain();
        
        this.lfo.connect(this.lfoDepthGain);
        this.lfoDepthGain.connect(this.lfoGain);
        this.lfoGain.connect(this.osc.frequency);
        
        this.osc.connect(this.gain).connect(masterGainNode);
        
        this.lfo.start();
        this.osc.start();
        
      } else if (this.type === 'right') {
        this.noiseSource = audioCtx.createBufferSource();
        this.noiseSource.buffer = whiteNoiseBuffer;
        this.noiseSource.loop = true;
        
        this.bandpass = audioCtx.createBiquadFilter();
        this.bandpass.type = 'bandpass';
        this.bandpass.Q.value = 10;
        
        this.gain = audioCtx.createGain();
        this.gain.gain.value = 0;
        
        this.reverbGain = audioCtx.createGain();
        this.dryGain = audioCtx.createGain();
        
        this.noiseSource.connect(this.bandpass);
        this.bandpass.connect(this.dryGain);
        this.bandpass.connect(this.reverbGain);
        this.reverbGain.connect(reverbNode);
        
        this.dryGain.connect(this.gain);
        this.gain.connect(masterGainNode);
        
        this.noiseSource.start();
      }
      
      debugLog(`‚úÖ Created ${this.type} sound layer #${this.id}`);
    }
    
    // Freeze the visual state when sustaining
    freezeVisualState(hand) {
      this.frozenPosition = {
        x: hand.x,  // Use direct x position to match mirrored camera
        y: hand.y
      };
      this.frozenFrequency = this.params.pitch.value;
      this.frozenColor = pitchToColor(this.frozenFrequency, 0.8);
      debugLog(`üßä Frozen visual state for layer #${this.id} at (${this.frozenPosition.x}, ${this.frozenPosition.y})`);
    }
    
    update(hand) {
      if (!hand && !this.sustained) {
        this.params.volume.update(0, 0.2);
        this.gain.gain.setTargetAtTime(this.params.volume.value, audioCtx.currentTime, 0.05);
        return;
      }
      
      // If sustained (frozen), don't update parameters - keep them locked
      if (this.sustained) {
        // Keep the frozen parameters active, don't follow hand movement
        if (this.type === 'left') {
          this.osc.frequency.setValueAtTime(this.params.pitch.value, audioCtx.currentTime);
          this.lfo.frequency.setValueAtTime(this.params.lfoRate.value, audioCtx.currentTime);
          this.lfoDepthGain.gain.setValueAtTime(parseFloat(ctrl.leftLfoDepth.value), audioCtx.currentTime);
          this.gain.gain.setTargetAtTime(this.params.volume.value, audioCtx.currentTime, 0.05);
        } else if (this.type === 'right') {
          this.bandpass.frequency.setValueAtTime(this.params.pitch.value, audioCtx.currentTime);
          const reverbWet = parseFloat(ctrl.rightReverbWet.value) * this.params.reverb.value;
          this.reverbGain.gain.setTargetAtTime(reverbWet, audioCtx.currentTime, 0.05);
          this.dryGain.gain.setTargetAtTime(1 - reverbWet, audioCtx.currentTime, 0.05);
          this.gain.gain.setTargetAtTime(this.params.volume.value, audioCtx.currentTime, 0.05);
        }
        return;
      }
      
      if (!hand) return;
      
      const smoothing = parseFloat(ctrl.smoothing.value);
      
      if (this.type === 'left') {
        const basePitch = parseFloat(ctrl.leftBasePitch.value);
        const pitchRange = parseFloat(ctrl.leftPitchRange.value);
        const targetPitch = basePitch + (1 - hand.y / height) * pitchRange;
        const pitch = this.params.pitch.update(targetPitch, smoothing);
        
        const targetLfoRate = (hand.x / width) * 20;
        const lfoRate = this.params.lfoRate.update(targetLfoRate, smoothing);
        
        const maxGain = parseFloat(ctrl.leftMaxGain.value) * parseFloat(ctrl.masterVolume.value);
        const targetVolume = hand.z * maxGain;
        const volume = this.params.volume.update(targetVolume, smoothing);
        
        this.osc.frequency.setValueAtTime(pitch, audioCtx.currentTime);
        this.lfo.frequency.setValueAtTime(lfoRate, audioCtx.currentTime);
        this.lfoDepthGain.gain.setValueAtTime(parseFloat(ctrl.leftLfoDepth.value), audioCtx.currentTime);
        this.gain.gain.setTargetAtTime(volume, audioCtx.currentTime, 0.05);
        
      } else if (this.type === 'right') {
        const minPitch = parseFloat(ctrl.rightMinPitch.value);
        const pitchRange = parseFloat(ctrl.rightPitchRange.value);
        const targetPitch = minPitch + (1 - hand.y / height) * pitchRange;
        const pitch = this.params.pitch.update(targetPitch, smoothing);
        
        const targetReverbMix = hand.x / width;
        const reverbMix = this.params.reverb.update(targetReverbMix, smoothing);
        
        const maxGain = parseFloat(ctrl.rightMaxGain.value) * parseFloat(ctrl.masterVolume.value);
        const targetVolume = hand.z * maxGain;
        const volume = this.params.volume.update(targetVolume, smoothing);
        
        this.bandpass.frequency.setValueAtTime(pitch, audioCtx.currentTime);
        
        const reverbWet = parseFloat(ctrl.rightReverbWet.value) * reverbMix;
        this.reverbGain.gain.setTargetAtTime(reverbWet, audioCtx.currentTime, 0.05);
        this.dryGain.gain.setTargetAtTime(1 - reverbWet, audioCtx.currentTime, 0.05);
        
        this.gain.gain.setTargetAtTime(volume, audioCtx.currentTime, 0.05);
      }
    }
    
    destroy() {
      debugLog(`üóëÔ∏è Destroying sound layer #${this.id}`);
      
      this.gain.gain.setTargetAtTime(0, audioCtx.currentTime, 0.02);
      
      setTimeout(() => {
        try {
          if (this.osc) this.osc.stop();
          if (this.lfo) this.lfo.stop();
          if (this.noiseSource) this.noiseSource.stop();
          
          this.gain.disconnect();
          if (this.lfoGain) this.lfoGain.disconnect();
        } catch (e) {}
      }, 50);
    }
  }

  // Enhanced hand processing with gesture recognition (unchanged)
  function processHands() {
    if (!predictions || predictions.length === 0) {
      return { left: null, right: null };
    }

    const hands = predictions.map(pred => {
      if (!pred.keypoints || pred.keypoints.length === 0) return null;
      
      let sx = 0, sy = 0;
      pred.keypoints.forEach(kp => { sx += kp.x; sy += kp.y; });
      
      // Scale coordinates from video resolution to canvas size
      const rawX = sx / pred.keypoints.length;
      const rawY = sy / pred.keypoints.length;
      
      // Get video actual dimensions for scaling
      const videoWidth = video.videoWidth || 1280;
      const videoHeight = video.videoHeight || 720;
      
      // Scale to canvas dimensions
      const scaledX = (rawX / videoWidth) * width;
      const scaledY = (rawY / videoHeight) * height;
      
      // Use new depth calculation with distance info
      const depthInfo = calculateHandDepthWithDistance(pred.keypoints);
      
      return { 
        x: scaledX, 
        y: scaledY,
        z: depthInfo.depth,
        distance: depthInfo.distance,
        keypoints: pred.keypoints,
        gesture: recognizeGesture(pred.keypoints),
        confidence: pred.confidence || 1.0,
        handedness: pred.handedness || pred.label || 'Unknown'
      };
    }).filter(hand => hand !== null);

    let left = null, right = null;

    if (hands.length === 1) {
      const hand = hands[0];
      if (hand.handedness === 'Left') {
        left = hand;
      } else if (hand.handedness === 'Right') {
        right = hand;
      } else {
        if (hand.x < width / 2) {
          right = hand;
        } else {
          left = hand;
        }
      }
    } else if (hands.length >= 2) {
      hands.forEach(hand => {
        if (hand.handedness === 'Left') {
          left = hand;
        } else if (hand.handedness === 'Right') {
          right = hand;
        }
      });
      
      if (!left && !right) {
        hands.sort((a, b) => a.x - b.x);
        right = hands[0];
        left = hands[hands.length - 1];
      }
    }

    // Update distance debug display
    let debugText = '';
    if (left) {
      debugText += `LEFT: ${left.distance.toFixed(1)}px ‚Üí ${(left.z * 100).toFixed(0)}% vol | `;
    }
    if (right) {
      debugText += `RIGHT: ${right.distance.toFixed(1)}px ‚Üí ${(right.z * 100).toFixed(0)}% vol`;
    }
    if (debugText) {
      distanceDebug.textContent = debugText;
    } else {
      distanceDebug.textContent = 'Show hands to see distances...';
    }

    // Debug coordinate scaling (log occasionally)
    if (frameCount % 120 === 0 && (left || right)) {
      const videoWidth = video.videoWidth || 1280;
      const videoHeight = video.videoHeight || 720;
      debugLog(`Coordinate scaling: Video(${videoWidth}x${videoHeight}) ‚Üí Canvas(${width}x${height})`);
    }

    return { left, right };
  }

  // Audio setup (unchanged)
  const audioCtx = new (window.AudioContext || window.webkitAudioContext)();
  let whiteNoiseBuffer, reverbNode;

  function createGlobalAudio() {
    masterGainNode = audioCtx.createGain();
    masterGainNode.connect(audioCtx.destination);
    
    const bufferSize = audioCtx.sampleRate * 2;
    whiteNoiseBuffer = audioCtx.createBuffer(1, bufferSize, audioCtx.sampleRate);
    const output = whiteNoiseBuffer.getChannelData(0);
    for (let i = 0; i < bufferSize; i++) {
      output[i] = Math.random() * 2 - 1;
    }
    
    reverbNode = audioCtx.createConvolver();
    const length = audioCtx.sampleRate * 2;
    const impulse = audioCtx.createBuffer(2, length, audioCtx.sampleRate);
    
    for (let channel = 0; channel < 2; channel++) {
      const channelData = impulse.getChannelData(channel);
      for (let i = 0; i < length; i++) {
        const n = length - i;
        channelData[i] = (Math.random() * 2 - 1) * Math.pow(n / length, 0.7);
      }
    }
    reverbNode.buffer = impulse;
    reverbNode.connect(masterGainNode);
    
    debugLog('‚úÖ Global audio nodes created');
  }

  // Gesture state tracking (unchanged)
  let gestureState = { left: 'none', right: 'none' };
  let sustainedSounds = { left: null, right: null };

  function handleGestures(hands) {
    let statusText = '';
    let layerText = [];
    
    ['left', 'right'].forEach(handType => {
      const hand = hands[handType];
      
      if (!hand) {
        gestureState[handType] = 'none';
        return;
      }
      
      const gesture = hand.gesture;
      const prevGesture = gestureState[handType];
      gestureState[handType] = gesture;
      
      statusText += `${handType.toUpperCase()}: ${gesture} | `;
      
      if (gesture === 'peace') {
        activeSounds.forEach(sound => sound.destroy());
        activeSounds.clear();
        sustainedSounds.left = null;
        sustainedSounds.right = null;
        layerText.push(`‚úåÔ∏è ${handType}: PEACE - All layers cleared & unfrozen`);
        debugLog(`‚úåÔ∏è Peace gesture: All sound layers destroyed and sustained sounds cleared`);
        return;
      }
      
      if (gesture === 'open' && prevGesture !== 'open') {
        const newId = ++soundIdCounter;
        const newSound = new SoundLayer(newId, handType, hand);
        newSound.active = true;
        activeSounds.set(newId, newSound);
        layerText.push(`ü§ö ${handType}: NEW sound #${newId}`);
      }
      
      if (gesture === 'fist') {
        let latestSound = null;
        let latestId = 0;
        
        activeSounds.forEach((sound, id) => {
          if (sound.type === handType && id > latestId) {
            latestSound = sound;
            latestId = id;
          }
        });
        
        if (latestSound && !latestSound.sustained) {
          // Freeze the sound at current parameters
          latestSound.sustained = true;
          latestSound.freezeVisualState(hand); // Capture position and visual properties
          sustainedSounds[handType] = latestSound;
          layerText.push(`‚úä ${handType}: FROZEN #${latestSound.id} at ${Math.round(latestSound.params.pitch.value)}Hz`);
          debugLog(`üßä Frozen sound layer #${latestSound.id} for ${handType} hand`);
        }
      }
      
      if (prevGesture === 'fist' && gesture === 'open') {
        // When going from fist to open, create a NEW layer (don't release the frozen one)
        const newId = ++soundIdCounter;
        const newSound = new SoundLayer(newId, handType, hand);
        newSound.active = true;
        activeSounds.set(newId, newSound);
        layerText.push(`ü§ö ${handType}: NEW layer #${newId} (frozen #${sustainedSounds[handType]?.id} continues)`);
        debugLog(`üÜï Created new sound layer #${newId} while keeping frozen layer #${sustainedSounds[handType]?.id}`);
      }
      
      if (prevGesture === 'fist' && gesture !== 'fist' && gesture !== 'open') {
        // Other transitions from fist (like partial) don't create new layers
        if (sustainedSounds[handType]) {
          layerText.push(`üëã ${handType}: Still frozen #${sustainedSounds[handType].id}`);
        }
      }
    });
    
    activeSounds.forEach((sound, id) => {
      const hand = hands[sound.type];
      sound.update(hand);
      
      if (!sound.sustained && sound.params.volume.value < 0.001) {
        sound.destroy();
        activeSounds.delete(id);
      }
    });
    
    gestureFeedback.textContent = statusText || 'Show hands to detect gestures...';
    soundLayers.innerHTML = layerText.length > 0 ? layerText.join('<br>') : 
      `Active layers: ${activeSounds.size} | Total created: ${soundIdCounter}`;
  }

  // Event listeners
  startBtn.onclick = () => {
    audioCtx.resume().then(() => {
      createGlobalAudio();
      startBtn.textContent = 'Audio & Visuals Started';
      startBtn.disabled = true;
      debugLog('Simplified audio and visual system initialized');
    });
  };

  // Main render loop with SIMPLIFIED EFFECTS
  function render() {
    frameCount++;
    
    // Draw video feed
    if (webcamRunning) {
      vctx.save(); 
      vctx.scale(-1, 1); 
      vctx.drawImage(video, -width, 0, width, height); 
      vctx.restore();
    }

    // Clear overlay canvas
    octx.clearRect(0, 0, width, height);
    
    // Draw edge detection motion tracking effect FIRST (background layer)
    drawEdgeDetectionEffect();
    
    const hands = processHands();
    
    // Handle gesture-based sound control
    if (masterGainNode) {
      handleGestures(hands);
    }
    
    // Update status
    const leftDetected = hands.left !== null;
    const rightDetected = hands.right !== null;
    
    if (leftDetected && rightDetected) {
      handDetectionStatus.textContent = `Both hands detected - Simplified mode active`;
    } else if (leftDetected) {
      handDetectionStatus.textContent = `Left hand: ${hands.left.gesture} gesture`;
    } else if (rightDetected) {
      handDetectionStatus.textContent = `Right hand: ${hands.right.gesture} gesture`;
    } else {
      handDetectionStatus.textContent = 'Show hands for gesture control';
    }

    // Simple hand visual feedback
    ['left', 'right'].forEach(handType => {
      const hand = hands[handType];
      if (!hand) return;
      
      // Get pitch-based color for this hand
      let frequency = 440;
      activeSounds.forEach(sound => {
        if (sound.type === handType) {
          frequency = sound.params.pitch.value;
        }
      });
      
      const color = pitchToColor(frequency, 0.8);
      const gestureEmoji = {
        'open': 'ü§ö',
        'fist': '‚úä', 
        'peace': '‚úåÔ∏è',
        'partial': 'üëã',
        'unknown': '‚ùì'
      }[hand.gesture] || '‚ùì';
      
      // Simple hand circle
      const handSize = parseInt(ctrl.handGlow.value);
      const glowIntensity = parseFloat(ctrl.effectIntensity.value);
      
      // Hand circle with simple glow
      octx.strokeStyle = color;
      octx.lineWidth = 3;
      octx.beginPath();
      octx.arc(hand.x, hand.y, handSize, 0, 2 * Math.PI);
      octx.stroke();
      
      // Inner fill based on volume
      const volume = hand.z * glowIntensity;
      if (volume > 0.1) {
        octx.fillStyle = pitchToColor(frequency, volume * 0.3);
        octx.beginPath();
        octx.arc(hand.x, hand.y, handSize * 0.7, 0, 2 * Math.PI);
        octx.fill();
      }
      
      // Gesture emoji
      octx.fillStyle = color;
      octx.font = 'bold 20px Arial';
      octx.fillText(gestureEmoji, hand.x - 10, hand.y + 7);
      
      // Simple info text
      octx.font = 'bold 11px Arial';
      octx.fillStyle = color;
      octx.fillText(`${handType.toUpperCase()}: ${Math.round(frequency)}Hz`, hand.x - 40, hand.y - 35);
      octx.fillText(`Vol: ${(hand.z * 100).toFixed(0)}%`, hand.x - 40, hand.y - 22);
    });

    // Draw all frozen sound layer circles
    drawFrozenLayers();

    requestAnimationFrame(render);
  }

  debugLog('Starting edge detection render loop');
  render();

})().catch(error => {
  console.error('‚ùå Fatal initialization error:', error);
  alert('Failed to initialize application: ' + error.message);
});
</script>
</body>
</html>